---
title: "Project 1: Multivariate Normality"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Project 1: Multivariate Normality}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(Willmottproject1)
```

# An Overview of My Package

My package contains several functions that test for normality. This vignette provides a quick overview of them and what they do.

# Testing for Univariate Normality

## myproptest()

One test for univariate normality is checking whether the actual proportion of data in an interval is consistent with normality. If the data is normally distributed, we would expect to see about 68.3% of the data within 1 standard deviation of the mean and about 95.4% of the data within 2 standard deviations of the mean. If the actual proportion of data within one of these intervals is more than three standard deviations away from the expected value, we would question whether the data is distributed normally. So, we would reject normality if:
$$|\hat p_{1} - 0.683| > 3\sqrt{\frac{(0.683)(1-0.683)}{n}} = \frac{1.396}{\sqrt{n} }$$
or
$$|\hat p_{2} - 0.954| > 3\sqrt{\frac{(0.954)(1-0.954)}{n}} = \frac{0.628}{\sqrt{n}} $$

The function myproptest() will use this logic to test for normality. When you input a vector of data, the function will calculate the actual proportion of data within 1 and 2 standard deviations and compare it to the expected proportions. It will output the 2 actual proportions, the 2 expected proportions, and a confidence interval for both proportions, since I feel it could be useful to see what the bounds are for normality for your situation. The "result" output will tell you whether your actual proportions fell in the confidence intervals--if one of the proportions fails, it will read "not consistent with normality". I also produced a histogram of the data with the standard deviations and proportions marked as a visual aid.

See myproptest() in action below:
```{r}
obj <- myproptest(psych$Indep)
obj$ci1sd
obj$ci2sd
obj$result
obj$plot
```


## myqqplot()

Another way of testing univariate normality is by constructing a Q-Q plot. To do this, we first order the data and calculate its corresponding probability level of $\frac{j-1/2}{n}$ (for the $j^{th}$ data point). We use these probability levels to calculate the theoretical quantiles, and then plot the ordered data points against the theoretical quantiles. If the data is normally distributed, we expect the points to lie close to a straight line.

In myqqplot() I create a Q-Q plot given a univariate dataset. I also added a line through the data so it would be easier for the user to compare the points. The function outputs only the plot. See an example of my function running below:

```{r}
obj <- myqqplot(psych$Benev)
obj$plot
```


## myrq()

Another way of testing univariate normality is by calculating the value of $r_Q$, the correlation coefficient for the ordered data and the theoretical quantiles. Once this value has been calculated, we can compare it to a table of values (such as the one shown in Table 4.2 of the Textbook) to determine whether to reject the assumption that the data is normally distributed. If the data is distributed normally, we would expect that $r_Q$ will be very close to 1.

Given a vector of data, my function creates the same Q-Q plot as myqqplot() and also calculates the correlation coefficient $r_Q$. See this function working below:

```{r}
obj <- myrq(psych$Supp)
obj$plot
```



# Bivariate Normality

## myresult47()

If two variables are distributed bivariate normal, then by Result 4.7, we would expect about 50% of the data to lie in the ellipse generated by
$$ (x-\mu)'S^{-1}(x-\mu) \le \chi^2_2(0.5)$$

myresult47() takes in two vectors and determines whether they are bivariate normal by calculating the percentage of data points that lie in the above region. Both the actual proportion of data and theoretical proportion of data (0.5) in this interval are outputted. In addition, a scatter plot of the two variables with the 50% ellipse is also produced. 

See an example of myresult47() below:
```{r}
obj <- myresult47(psych$Conform,psych$Leader)
obj$plot
```


# Multivariate Normality

## mychisqplot()

One test for multivariate normality is creating a Chi-Squared Plot. To do this, we must first calculated the standardized squared distances for each observation $x_j$ using
$$d_j^2 = (x_j - \bar x)' S^{-1} (x_j - \bar x)$$

We can then plot the $d_j^2$ against the $\frac{100(j-1/2)}{n}$ quantile of $\chi^2_p$. If the data is distributed normally, then the points should lie close to the line $y=x$. It should also be true that approximately half of the $d_j^2$ will be less than or equal to $qchisq(0.50,p)$.

In mychisqplot(), we can input a matrix of data to create the corresponding chi-square plot and calculate the proportion of $d_j^2$ less than or equal to $qchisq(0.50,p)$. We can see this function working below:
```{r}
obj <- mychisqplot(psych[,1:5])
obj$propdj
obj$expprop
obj$plot
```


## mydetectout()

mydetectout() follows the following steps to detect outliers in the data:

1. Making a dot plot for each variable

2. Making a scatter plot for each pair of variables 

3. Examine the standardized values $z_{jk} =\frac{(x_{jk} - \bar x_k)}{\sqrt(s_kk)}$ for abnormal values. In my function, the "coordinates" of data with $|z_{jk}| > 3$ are pulled out for closer examination.

4. Examine the generalized squared distances $d_j^2$ for large values. In my function, row of data with $d_j^2 > qchisq(0.95,p)$ are pulled out for closer examination. 

See this function working below:
```{r}
obj <- mydetectout(psych[,1:5])
obj$dotplots
obj$scatters
obj$zoutliers
obj$gsdoutliers
```


# myboxcox()

If your data is not distributed normally, it may be possible to transform it into something a bit more normal using a Box-Cox Transformation:
$$ x^{(\lambda)} =  \begin{cases} 
      \frac{x^{\lambda} - 1}{\lambda} & \lambda \ne 0, x>0 \\
      ln(x) & \lambda = 0, x>0
   \end{cases}$$
To use this transformation, we need to find the value of $\lambda$ that maximizes the log-likelihood function 
$$l(\lambda) = \frac{-n}{2}[\sum_{j=1}^n (x_j^{(\lambda)}-mean(x^{(\lambda)}))^2] + (\lambda - 1)\sum_{j=1}^n ln(x_j)$$

myboxcox() takes in a vector of data, a vector of values for lambda, the number of iterations, and a confidence level. Using the bootstrap method, it creates a sample of the data and determines the value of lambda for each iteration. Once the iterations are complete, the mean value of lambda is considered the "best lambda" and a confidence interval for the best value of lambda is formed. This information is also shown in the plot of the log likelihood function.

See myboxcox() working below:
```{r}
obj <- myboxcox(psych$Conform)
obj$plot
```



# Example 4.39

The following questions refer to the packaged dataset "psych"

## Examine each of the variables independence, support, benevolence, conformity and leadership for marginal normality

I will first start by looking at the results of the Proportion Test for normality for each of the variables. All five variables had results that were consistent with normality.

```{r}
myproptest(psych$Indep)$result
myproptest(psych$Supp)$result
myproptest(psych$Benev)$result
myproptest(psych$Conform)$result
myproptest(psych$Leader)$result
```

I will also look at the $r_Q$ values for each of these variables. All of these values are pretty high, so we would expect them to lie close to a straight line. According to table 4.2, at a 95% confidence level, we would reject normality if $r_Q$ fell below 0.9873 (n=100) or 0.9913 (n=150). Since n=130 in this case, the rejection value will lie somewhere in this range, so the "independence" and "support" $r_Q$ values could be a low depending on the true value. Also, the "leadership" value is lower than this entire range, so we would reject normality for this variable.
```{r}
myrq(psych$Indep)$rq
myrq(psych$Supp)$rq
myrq(psych$Benev)$rq
myrq(psych$Conform)$rq
myrq(psych$Leader)$rq
```

Based on all this information, I would conclude that benevolence and leadership are distributed normally and independence, support, and leadership are approximately normal.

## Using all 5 variables, check for multivariate normality

I can check for multivariate normality using a Chi-Square plot. The actual proportion of data with $d_j^2 <= qchisq(0.5,5)$ is 0.492, which is very close to 0.5, the expected proportion. In addition, the points on the Chi-Square plot lie very close to the line. Therefore, it seems as though the variables are distributed multivariate normal.

```{r}
mychisqplot(psych[,1:5])$plot
mychisqplot(psych[,1:5])$propdj
```

## For those variables that are non-normal, determine the transformation that makes them more normal

The three variables that weren't completely normal were independence, support, and leadership. I will use the Box-Cox transformation to determine the best value of lambda to use to make these variables more nearly normal. From the below output, we see that Box-Cox transformations with
$\lambda_{Indep} = 0.4861$, $\lambda_{Supp} = 1.3925$ and $\lambda_{Leader} = 0.3652$ would make the data more normal.
```{r}
set.seed(35)
myboxcox(psych$Indep)$bestlam
myboxcox(psych$Supp)$bestlam
myboxcox(psych$Leader)$bestlam
```

